# UCSD Pick and Place Dataset

This dataset contains **1355** robot trajectories across two tasks: (1) picking up a pot and placing it in a sink within a kitchen environment (*200 trajectories*), and (2) picking up objects in a simple tabletop environment (*1155 trajectories*). The robot is a xArm7 robot with a two-finger gripper. The dataset is collected by a variety of policies, including human teleoperation, imitation learning, and reinforcement learning, and contains both successful and unsuccesful trajectories. Trajectories contain slight variations in initial configuration, camera pose, lighting conditions, and objects. All trajectories include (sparse) per-step reward labels generated by a combination of human annotation and a classifier.

----
<p align="center">
    <span style="font-weight: bold">Kitchen pick and place</span>
    <br/>
    <img src="figures/00.png" width="19.5%" />
    <img src="figures/01.png" width="19.5%" />
    <img src="figures/02.png" width="19.5%" />
    <img src="figures/03.png" width="19.5%" />
    <br/>
    <img src="figures/10.png" width="19.5%" />
    <img src="figures/11.png" width="19.5%" />
    <img src="figures/12.png" width="19.5%" />
    <img src="figures/13.png" width="19.5%" />
</p>

----
<p align="center">
    <span style="font-weight: bold">Tabletop picking</span>
    <br/>
    <img src="figures/20.png" width="19.5%" />
    <img src="figures/21.png" width="19.5%" />
    <img src="figures/22.png" width="19.5%" />
    <img src="figures/23.png" width="19.5%" />
    <br/>
    <img src="figures/30.png" width="19.5%" />
    <img src="figures/31.png" width="19.5%" />
    <img src="figures/32.png" width="19.5%" />
    <img src="figures/33.png" width="19.5%" />
</p>

